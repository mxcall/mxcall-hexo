---
title: K8S学习笔记(极客时间01-15)
date: 2020-05-25 00:30:18
toc: true
categories: [工作, 程序员, k8s]
tags: [k8s, docker]
index_img: https://gitee.com/matrixcall/bed001/raw/master/img/2020/05/20200528160755.png
---

 
 
## 小鲸鱼大事记
 
 
### docker 与传统Paas区别
 
 
 
 
```
其实没区别, 都是用Cgroups和Namespace去隔离应用环境, 但是Docker凭借:
简单的打包方式 + 本地环境与云端环境高度一致 + 以开发者为核心(简单化)
就这个3个小的tip打败了如日中天的Cloud Foundry(Paas代表), 衰败的原因是因为:
想凭借docker撼动传统大厂, 结果被k8s打败...
容器本身价值不大,有价值的是容器编排
 
 
 
 
```
 
 
## 白话容器基础
 
 
> 容器本质: **只是一种特殊的进程**
 
 
> 容器设计理念: **容器和应用能够同生命周期**
 
 
> 用cgroup 控制资源 ,用namespace 控制该进程对 其他进程的视图
 
 
> namespace构建出进程的围墙;cgroup构建出进程的天空; rootfs和mount namespace 构建出进程的大地
 
 
### 核心技术namespace
 
 
 
 
```
核心作用: 是修改进程视图,
本质上是: 创建进程时的一个参数;设置该进程对其他进程的可视边界;
除了进程的pid namespace, 还有Mount、UTS、IPC、Network 和 User 这些namespace
 
 
```
 
 
### 核心技术cgroup
 
 
 
 
```
核心作用:根据cgroup的目录内容, 来控制具体某个进程使用的 各种资源(cpu内存等等)
mount -t cgroup
 
 
缺点:
1. 隔离性不高,如top命令查看的内存cpu仍然是宿主机的而不是容器配置的;如修改时间会影响宿主机时间;
 
缺点修复:
1. 用lxcfs(linux container file system)修复top以及free与容器资源不一致的问题
 
 
```
 
 
### 容器镜像=rootfs=根文件系统
 
 
```
- 其中包含除了内核外 （/boot文件夹下面）的所有系统文件
- 所以修改内核相关操作会直接影响 宿主机
```
 
 
可以在宿主机找到容器运行的具体目录
 
 
```
1. docker ps
2. docker inspect xx
3. 找到GraphDriver
"GraphDriver": {
        "Data": {
            "LowerDir": "/var/docker/docker-root/overlay2/55c6a4e-init/diff:/var/docker/docker-root/overlay2/848abf14",
            "MergedDir": "/var/docker/docker-root/overlay2/55c6a4e/merged",
            "UpperDir": "/var/docker/docker-root/overlay2/55c6a4e/diff",
            "WorkDir": "/var/docker/docker-root/overlay2/55c6a4e/work"
        },
        "Name": "overlay2"
}
4. 进入MergedDir目录即可找到
5. 其中
LowerDir:  镜像层;即image层, 只读
UpperDir:  容器层,指运行时产生的文件, 可读写
MergedDir: 挂载了LowerDir和UpperDir后的真正文件视图;
WorkDir:   用来实现copy_up操作, 暂时没有测试出来,
```
 
 
### 有个xxxx-init层
 
 
```
挂载一些/etc/hosts ,/etc/hostname, /etc/resolv.conf等文件;
这些文件的修改不会提交(docker commit)到真正的镜像仓库中;
并且可以通过docker run --参数来动态的启动时修改;
```
 
 
### 文件系统分层技术
 
 
```
1. 增加: 即在容器层UpperDir添加文件;
2. 修改: 若修改只读层数据LowerDir, 则要copy-on-write, 即写的时候复制到容器层, 后续打成镜像后会覆盖只读层的同名文件, 会影响写入效率; 若修改容器层UpperDir数据, 则直接修改;
3. 查找: 根据对应的docker驱动, 一层层向下查找
4. 删除: 通过whiteout(白障)技术, 新建一个同名的0kb的文件, 即仅仅在上层标记它删除,
这就是为什么容器镜像会做的比较大的原因, 因为如果你不在当层把无关文件删除, 后续就没有机会删除了,仅仅是标记删除
```
 
 
 
 
 
### Volume
 
 
```
Dockerfile里面的 VOLUME /xx 和 docker run -v /xx
这种写法都是强制挂载的意思, 以前只写过-v /xxx:/test;
简单的说, 就是强行在宿主机上新建个随机uuid名字的文件夹, 然后挂载到容器里面, 容器里面这个目录的读写全部会映射到宿主机上的随机uuid目录;
可以用 docker run -v /xxx:/test  来覆盖这个写法, 即指定具体的宿主机目录,而不是随机uuid目录;
```
 
 
### EntryPoint
 
 
```
默认即不指定时是隐含的, 值为/bin/sh -c "/bin/bash -l"
 
 
```
 
 
 
 
 
 
![](https://cdn.jsdelivr.net/gh/matrixcall/bed001@master/img/2019/01/20191216144754.png)

 
 
 
 
 
![](https://cdn.jsdelivr.net/gh/matrixcall/bed001/img/2020/05/20200524141649.png)
 
 
 
 
 
## 从容器到容器云
 
 
> CNI（Container Networking Interface）和 CSI（Container Storage Interface）
 
 
### 个人思考:
 
 
```
代码级别的面向对象 提升到 架构级别;如组合设计模式(devops),
描述的容器间的依赖关系(更细粒度)编排 , 而不仅仅是调度(分配到合适机器上);
整套解决方案CNCF;
 
 
```
 
 
 
 
 
 
### 声明式api 和 命令式api
 
 
```
1. 声明式example: 告诉机器想要的是什么, 面向对象编程! 项目经理, 中午聚餐, 你定个地方; SQL, Lambda, antdesign(js), 给参数or属性
已存在通用算法去解决问题，所以程序员只需要准确地描述待解决的任务就可以了，而不需要描述一个算法去解决问题。
 
2. 命令式example: 告诉机器如何去做, 面向过程编程 ! 架构师程序员, 中午聚餐, 你.......;存储过程, 手动for循环, html(tr,td), 给具体的method方法
基于伪代码和机器语言的传统方法。即How: 发现解决问题的算法，再用指令序列来表达这个算法, 适用于设计一个特定程序来解决问题;
```
 
 
 
 
 
### kubernetes成功原因: (降维打击)
 
 
```
所以，相比于“小打小闹”的 Docker 公司、“旧瓶装新酒”的 Mesos 社区，Kubernetes 项目从一开始就比较幸运地站上了一个他人难以企及的高度：在它的成长阶段，这个项目每一个核心特性的提出，几乎都脱胎于 Borg/Omega 系统的设计与经验(本来不可能开源的)。更重要的是，这些特性在开源社区落地的过程中，又在整个社区的合力之下得到了极大的改进，修复了很多当年遗留在 Borg 体系中的缺陷和问题。
从一开始，Kubernetes 项目就没有像同时期的各种“容器云”项目那样，把 Docker 作为整个架构的核心，而仅仅把它作为最底层的一个容器运行时实现。运行在大规模集群中的各种任务之间，实际上存在着各种各样的关系(ds,deploy)。这些关系的处理，才是作业编排和管理系统最困难的地方。
Kubernetes 项目最主要的设计思想是，从更宏观的角度，以统一的方式来定义任务之间的各种关系，并且为将来支持更多种类的关系留有余地。
```
 
 
 
 
 
 
### 调度 和 编排 区别
 
 
```
过去很多的集群管理项目（比如 Yarn、Mesos，以及 Swarm）所擅长的，都是把一个容器，按照某种规则，放置在某个最佳节点上运行起来。这种功能，我们称为“调度”。
 
而 Kubernetes 项目所擅长的，是按照用户的意愿和整个系统的规则，完全自动化地处理好容器之间的各种关系。这种功能，就是我们经常听到的一个概念：编排。
```
 
 
## 搭建集群
 
 
### yaml执行
 
 
```
作为用户, 我不管是create -f 还是 replace -f, 始终用
kubectl apply -f xx.yaml
```
 
 
### yaml对象
 
 
```
一般由metadata + spec 组成
 
```
 
###  根据label过滤
 
```
kubectl get pods -l app=nginx
```
 
 
 
 
 
 
### 相关坑:
 
 
```
https://blog.csdn.net/liukuan73/article/details/83116271
https://www.jianshu.com/p/b77635125524
 
 
# 查询apt版本
apt-cache madison xxx
apt-get install docker-ce=上面查询的版本
# 修改hostname
hostnamectl set-hostname k8s-02
vim /etc/hosts
# 提前拉镜像
kubeadm config images list
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
# 初始化
kubeadm init --kubernetes-version=v1.13.4
kubeadm init --apiserver-advertise-address=10.1.100.11 --pod-network-cidr=172.16.0.0/16 --service-cidr=10.233.0.0/16 --kubernetes-version=v1.12.7
kubeadm join 10.1.100.11:6443 --token l5xqm7.bzegizogtovree1v --discovery-token-ca-cert-hash sha256:cc4e1812fa98c98c881470cb46c2d77c6f0596927eb13b6ad6da59a2f5a50d9f
 
 
# 镜像加速/etc/docker/daemon.json
{
    "registry-mirrors": ["https://u4ag1zcm.mirror.aliyuncs.com"],
    "insecure-registries": [
        "hub.dz11.com",
        "dyhub.douyucdn.cn",
        "upload.docker-registry1.ocean.douyu.com",
        "docker-registry1.ocean.douyu.com"
    ]
}
# 代理
mkdir /etc/systemd/system/docker.service.d
vim  /etc/systemd/system/docker.service.d/http-proxy.conf
[Service]
Environment="HTTP_PROXY=http://10.1.0.2:1081"
Environment="HTTPS_PROXY=http://10.1.0.2:1081"
Environment="NO_PROXY=localhost,127.0.0.0/8,10.1.0.0/16,172.16.0.0/16,10.233.0.0/16,hub.docker.com,registry.cn-hangzhou.aliyuncs.com"
# 网络插件
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
# iptables
net.ipv4.ip_forward=1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
>sysctl -p
报错sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directory
>modprobe br_netfilter
>sysctl --system
 
 
 
 
 
 
```
 
 
 
 
 
## StatefulSet
 
### 测试容器内网络
 
```
kubectl run -it --rm  --restart=Never --image busybox:1.28.4 test-dns /bin/sh
环境(一个master, 一个node, coredns)
1. busybox的版本要控制在1.28.4以下
kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh
2. 主从节点时间要一致
3. 从节点拷贝主节点的admin.conf文件到从节点
4. 重启kubelet docker
完了之后发现 两天解析不到的dns 竟然成功了
```
 
### pv, pvc是如何找到他对应的pv的
 
```
试着回答一下：
1、 指定了storageClassName的pv, 只有指定了同样storageClassName的pvc才能绑定到它上面。
2、对于没有指定storageClassName的pv,默认classname为"", 同样只有没有指定classname的pvc才能绑定到它上面。
3、pvc可以用matchLabels和matchExpressions来指定标签完成更详细的匹配需求。
4、匹配成功应该还需要一些基本的存储条件吧，比如pvc申请的存储空间肯定不能大于指定的pv.
 
```
 
### subPath
 
```
即可以选择挂载覆盖 pod里面的 子文件夹或文件, 而不影响 pod里面 同级的其他文件夹或文件, 如下,subpath的含义代表configmap中的子文件resolve.conf,而不是pod里面的实际目录文件;
若subPath为目录, 也是代表configmap或外置存储中的子文件夹(包含本身)
 
   volumeMounts:
   - name: host-log-dir
     mountPath: /logs
   - name: dns-overwrite-mount-point
     mountPath: /etc/resolv.conf
     subPath: resolv.conf
volumes:
- name: host-log-dir
   hostPath:
     path: /docker-logs/applogs/cradle
- name: dns-overwrite-mount-point
   configMap:
     name: bigdata-dns
#####################################
spec:
    containers:
    - name: mysql
      image: mysql
      env:
      - name: MYSQL_ROOT_PASSWORD
        value: "rootpasswd"
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: site-data
        subPath: mysql
    - name: php
      image: php:7.0-apache
      volumeMounts:
      - mountPath: /var/www/html
        name: site-data
        subPath: html
    volumes:
    - name: site-data
      persistentVolumeClaim:
        claimName: my-lamp-site-data
```
 
 
 
### TODO:PVC挂载多POD(目录区分?)
 
 
 
## 深入解析Pod对象
 
 
 
> Pod 这种“超亲密关系”容器的设计思想，实际上就是希望，当用户想在一个容器里跑多个功能并不相关的应用时，应该优先考虑它们是不是更应该被描述成一个 Pod 里的多个容器。
 
> Pod实际上是在扮演传统基础设施里“虚拟机”的角色；而容器，则是这个虚拟机里运行的用户程序。(方便上云模拟进程组)
 
>pod是一个小家庭，它把密不可分的家庭成员(container)聚在一起，Infra container则是家长，掌管家中共通资源，家庭成员通过sidecar方式互帮互助，其乐融融～
 
### POD存在的意义
 
```
1. POD是一组容器的逻辑概念;
2. 出现的原因是进程之间 有拓扑关系 即进程组(tree), 如文件交换,本地通信, 频繁的远程调用, 缓存共享等等;
3. 超亲密关系;
4. Swarm 失败的原因之一, 不支持pod这种多容器组合运行;
4. Pod 扮演的是传统部署环境里“虚拟机”的角色。这样的设计，是为了使用户从传统环境（虚拟机环境）向 Kubernetes（容器环境）的迁移，更加平滑, 把容器看作是运行在这个“机器”里的“用户程序”。
```
 
### POD实现原理
 
```
1. Pod 里的所有容器，共享的是同一个 Network Namespace，并且可以声明共享同一个 Volume;
2. Infra容器 = 空容器 = k8s.gcr.io/pause, 其他容器都加入到这个容器的相关namespace中来;
3. 所以,可以想象所有的容器都在一个大的 容器下面跑, 共享ip, 共享通信, 共享挂载的volume;
 
```
 
 
 
 
 
### 容器设计模式
 
```
实际上就是 少用 继承 , 多用组合
1. 组合模式 = sidecar
2.
 
```
 
### Init Container
 
```
在 Pod 中，所有 Init Container 定义的容器，都会比 spec.containers 定义的用户容器先启动。并且，Init Container 容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动。
```
 
 
 
### 组合模式Demo
 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: javaweb-2
spec:
  initContainers:
  - image: geektime/sample:v2
    name: war
    command: ["cp", "/sample.war", "/app"]
    volumeMounts:
    - mountPath: /app
      name: app-volume
  containers:
  - image: geektime/tomcat:7.0
    name: tomcat
    command: ["sh","-c","/root/apache-tomcat-7.0.42-v2/bin/start.sh"]
    volumeMounts:
    - mountPath: /root/apache-tomcat-7.0.42-v2/webapps
      name: app-volume
    ports:
    - containerPort: 8080
      hostPort: 8001
  volumes:
  - name: app-volume
    emptyDir: {}
```
 
### POD生命周期
 
```
1. postStart(并行)的执行时机 和 Docker 中的entrypoint 是并行的, 并没有严格的区分先后顺序;
若执行失败, pod的状态也为失败;
2. preStop(串行), 收到sigkill信号后执行, 会阻塞杀死的过程;
+++++++++++
3. pod状态存储在pod.status.phase中, 其他的都是对这个状态的细节描述, 主要有
Pending。这个状态意味着，Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod 里有些容器因为某种原因而不能被顺利创建。比如，调度不成功。
Running。这个状态下，Pod 已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中。
Succeeded。这个状态意味着，Pod 里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。
Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状态的出现，意味着你得想办法 Debug 这个容器的应用，比如查看 Pod 的 Events 和日志。
Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。
 
++++++++++++
 
```
 
 
 
### **投射键值对**
 
```
1. 投射目录有4种: Secret, ConfigMap, Downward API, ServiceAccountToken(属于Secret)
2. 若secret 和 configmap变更, 那么挂载进入pod的文件也会自动跟着变更, 应用不用自动重启,
但是若以环境变量的形式注入, 则不会自动更新!!!!
3. secret 默认用base64编码... 可被逆转, echo -n 'admin' | base64
4. 常用命令: (from文件可以指导我们 怎么样编写多行内容的yaml + 自动base64)
# 创建
kubectl create secret generic user --from-file=./username.txt
kubectl create configmap ui-config --from-file=example/ui.properties
 
5. 内置api, 一定是 Pod 里的容器进程启动之前就能够确定下来的信息;
# 使用 fieldRef 可以声明使用:
spec.nodeName - 宿主机名字
status.hostIP - 宿主机 IP
metadata.name - Pod 的名字
metadata.namespace - Pod 的 Namespace
status.podIP - Pod 的 IP
spec.serviceAccountName - Pod 的 Service Account 的名字
metadata.uid - Pod 的 UID
metadata.labels['<KEY>'] - 指定 <KEY> 的 Label 值
metadata.annotations['<KEY>'] - 指定 <KEY> 的 Annotation 值
metadata.labels - Pod 的所有 Label
metadata.annotations - Pod 的所有 Annotation
 
# 使用 resourceFieldRef 可以声明使用:
容器的 CPU limit
容器的 CPU request
容器的 memory limit
容器的 memory request
```
 
 
 
### POD状态
 
```
1.但一定要强调的是，Pod 的恢复过程，永远都是发生在当前节点上，而不会跑到别的节点上去。事实上，一旦一个 Pod 与一个节点（Node）绑定，除非这个绑定发生了变化（pod.spec.node 字段被修改），否则它永远都不会离开这个节点。这也就意味着，如果这个宿主机宕机了，这个 Pod 也不会主动迁移到其他节点上去。
2.而如果你想让 Pod 出现在其他的可用节点上，就必须使用 Deployment 这样的“控制器”来管理 Pod，哪怕你只需要一个 Pod 副本
 
3.只要 Pod 的 restartPolicy 指定的策略允许重启异常的容器（比如：Always），那么这个 Pod 就会保持 Running 状态，并进行容器重启。否则，Pod 就会进入 Failed 状态 。
 
4.对于包含多个容器的 Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态。
```
 
 
 
### ServiceAccountToken
 
```
# 经过测试无法通过kubectl直接操作, 估计还必须得写程序折腾
一旦 Pod 创建完成，容器里的应用就可以直接从这个默认 ServiceAccountToken 的挂载目录里访问到授权信息和文件。这个容器内的路径在 Kubernetes 里是固定的，
所以，你的应用程序只要直接加载这些授权文件，就可以访问并操作 Kubernetes API 了。而且，如果你使用的是 Kubernetes 官方的 Client 包（k8s.io/client-go）的话，它还可以自动加载这个目录下的文件，你不需要做任何配置或者编码操作。
 
 
```
